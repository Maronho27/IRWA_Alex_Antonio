{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["YmAgPsFwkUox","X0UIXhCGZJzL"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbHIob8C6qlq","executionInfo":{"status":"ok","timestamp":1667932351588,"user_tz":-60,"elapsed":2110,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"b7f26fe3-6918-4c0f-c71f-94d1a8e9dc29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount the drive for latter importing the datasets\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#**1. Import dictionaries**"],"metadata":{"id":"iUdATAJvjf0w"}},{"cell_type":"code","source":["# download nltk and the stopwords\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2UjO36z7dDl","executionInfo":{"status":"ok","timestamp":1667932351590,"user_tz":-60,"elapsed":13,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"da6ba7af-b3e9-4c52-8816-daf57d37f16e"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["# download the rest of dictionaries\n","from collections import defaultdict\n","from array import array\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import math\n","import numpy as np\n","import collections\n","from numpy import linalg as la\n","import time\n","\n","#new dictionaries for this lab\n","import pandas as pd\n","from gensim.models.word2vec import Word2Vec\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","# We have added these dictionaries to the ones that were included in lab 1\n","import json\n","import re\n","import csv"],"metadata":{"id":"S4Ja2y3-7iWL","executionInfo":{"status":"ok","timestamp":1667932351591,"user_tz":-60,"elapsed":9,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":["#**2. Load the datasets**"],"metadata":{"id":"UBOrO5Gzj0DT"}},{"cell_type":"code","source":["docs_path = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tw_hurricane_data.json'\n","tweets = []\n","# open the JSON file\n","with open(docs_path) as fp:\n","    for jsonObj in fp:\n","        tweetsDict = json.loads(jsonObj)\n","        tweets.append(tweetsDict) # add the tweets in our array tweets"],"metadata":{"id":"6CylZgIYM9b4","executionInfo":{"status":"ok","timestamp":1667932352469,"user_tz":-60,"elapsed":886,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["docs_path_2 = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tweet_document_ids_map.csv'\n","doc_id = {}\n","# open the CSV file\n","with open(docs_path_2, newline='') as csvfile:\n","  spamreader = csv.reader(csvfile, delimiter=' ', quotechar=' ')\n","  for row in spamreader:\n","    doc_id[row[0].split()[1]] = row[0].split()[0] # add the doc number as an entry of our dictionary, having the tweet id as the key of this entry"],"metadata":{"id":"wNoB7lcV7j15","executionInfo":{"status":"ok","timestamp":1667932352470,"user_tz":-60,"elapsed":16,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["print(row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6DYSVs5wMxt","executionInfo":{"status":"ok","timestamp":1667932352470,"user_tz":-60,"elapsed":15,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"22f0e3de-fa18-4f4b-a05f-64a27655eef4"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["['doc_4000\\t1575856225908326400']\n"]}]},{"cell_type":"markdown","source":["#**3. Text Processing**"],"metadata":{"id":"YmAgPsFwkUox"}},{"cell_type":"code","source":["def build_terms(tweet):\n","    \"\"\"\n","    Preprocess the text of the tweet by eliminating the url, the people labelled with the @,\n","    eliminating the punctuation, separating the words after the hashtag, removing stop words, \n","    stemming, transforming in lowercase and returning the tokens of the text.\n","    \n","    Argument:\n","    tweet -- string (text) to be pre-processed\n","    \n","    Returns:\n","    tweet - a list of tokens corresponding to the input text after the pre-processing\n","    \"\"\"\n","\n","    stemmer = PorterStemmer() # stemm the words to get the root of the word and avoid having different words that mean the same\n","    stop_words = set(stopwords.words(\"english\")) # eliminate all the stop words to make efficient queries and documents\n","    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # separate the words without including puntuation marks\n","    \n","    tweet = re.sub(r'http\\S+', '', tweet) ## delete the url\n","    tweet = re.sub(r'@\\S+', '', tweet) ## delete the word after @ (so the people labelled)\n","    tweet = \" \".join([a for a in re.split('([A-Z][a-z]+)', tweet) if a]) ## separate the hashtags in words according to the capital letters\n","    tweet = tweet.replace(\"_\", \" \") ## eliminate the _ (it is the only punctuation mark that is not deleted with tokenize)\n","    tweet = tweet.lower() ## transform in lowercase\n","    tweet = tokenizer.tokenize(tweet) ## tokenize the text to get a list of terms and remove punctuation marks\n","    tweet=[i for i in tweet if i not in stop_words]  ## eliminate the stopwords\n","    tweet=[stemmer.stem(i) for i in tweet] ## perform stemming\n","\n","    return tweet"],"metadata":{"id":"y4OC8L7s_7Oc","executionInfo":{"status":"ok","timestamp":1667932352471,"user_tz":-60,"elapsed":10,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["def create_index(tweets):\n","  \"\"\"\n","  Create the inverted index and the tweets dictionary\n","\n","  Argument:\n","  tweets -- collection of tweets\n","  \n","  Returns:\n","  index -- the inverted index. Contains the terms as keys and in which tweets (appear as the document number related to the tweet id)\n","  and in which position inside this tweet appears each term\n","  tweets-index -- the tweet's dictionary. Contains an entry for each tweet which key is the document number related with\n","  the tweet's id. Each tweet has its text, username, date, hashtags, number of likes, number of retweets and url if they exist \n","  \"\"\"\n","  index = defaultdict(list) # We create the inverted index\n","  tweets_index = {} # We create the tweets dictionary\n","  counter = 0\n","\n","  for tweet in tweets:\n","    # for each tweet we create a dictionary containing the text, username, date, hashtags, number of likes, number of retweets and url if they exist\n","    tweet_dict = {}\n","    try:\n","      tweet_dict[\"text\"] = tweet[\"full_text\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"username\"] = tweet['user']['screen_name']\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"date\"] = tweet[\"created_at\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"hashtags\"] = []\n","      for i in range(0, len(tweet[\"entities\"][\"hashtags\"])):\n","        tweet_dict[\"hashtags\"].append(tweet[\"entities\"][\"hashtags\"][i])\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"likes\"] = tweet[\"favorite_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"retweets\"] = tweet[\"retweet_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"url\"] = tweet[\"entities\"][\"media\"][0][\"url\"]\n","    except:\n","      pass\n","\n","    tweets_index[doc_id[str(tweet[\"id\"])]] = tweet_dict # save the tweet in tweets index by the document number related with the tweet id\n","\n","    terms = build_terms(tweet[\"full_text\"]) # call build terms for processing the text of the tweet\n","\n","    if counter <= 10:\n","      # print the tweet text and terms for checking it the result is okay (now we only do this for the first tweet but before delivering we have checked more tweets)\n","      print(\"Original full text of the tweet': \\n{}\".format(tweet[\"full_text\"]))\n","      print(\"Terms after processing the text': \\n{}\".format(terms))\n","      counter += 1\n","\n","    current_page_index = {}\n","\n","    for position, term in enumerate(terms): # loop over all terms\n","        try:\n","            # if the term is already in the index for the current page append the position\n","            current_page_index[term][1].append(position)\n","        except:\n","            # else add the new term as dict key and set the document number corresponding to this tweet and the position where the term appears in this tweet\n","            current_page_index[term]=[doc_id[str(tweet[\"id\"])], array('I',[position])] #'I' indicates unsigned int (int in Python)\n","        \n","    #merge the current page index with the main index\n","    for term_page, posting_page in current_page_index.items():\n","        index[term_page].append(posting_page)\n","    \n","  return index, tweets_index"],"metadata":{"id":"GZxG9cENPTlM","executionInfo":{"status":"ok","timestamp":1667932352472,"user_tz":-60,"elapsed":9,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":["#**4. Check the results**"],"metadata":{"id":"X0UIXhCGZJzL"}},{"cell_type":"code","source":["start_time = time.time()\n","index, tweets_index = create_index(tweets) # run create_index() for creating the inverted index and the tweets index\n","print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2))) # calculate how much time does the process last"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-ZvsfJHaKF9","executionInfo":{"status":"ok","timestamp":1667932355514,"user_tz":-60,"elapsed":3051,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"cf2135d8-03bb-4bb9-eb23-bb580190cfee"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["Original full text of the tweet': \n","So this will keep spinning over us until 7 pm‚Ä¶go away already. #HurricaneIan https://t.co/VROTxNS9rz\n","Terms after processing the text': \n","['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', 'hurrican', 'ian']\n","Original full text of the tweet': \n","Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. üíô\n","Terms after processing the text': \n","['heart', 'go', 'affect', 'hurrican', 'ian', 'wish', 'everyon', 'road', 'current', 'brave', 'condit', 'safe', 'travel']\n","Original full text of the tweet': \n","Kissimmee neighborhood off of Michigan Ave. \n","#HurricaneIan https://t.co/jf7zseg0Fe\n","Terms after processing the text': \n","['kissimme', 'neighborhood', 'michigan', 'ave', 'hurrican', 'ian']\n","Original full text of the tweet': \n","I have this one tree in my backyard that scares me more than the poltergeist tree when it‚Äôs storming and windy like this. #scwx #HurricaneIan\n","Terms after processing the text': \n","['one', 'tree', 'backyard', 'scare', 'poltergeist', 'tree', 'storm', 'windi', 'like', 'scwx', 'hurrican', 'ian']\n","Original full text of the tweet': \n","@AshleyRuizWx @Stephan89441722 @lilmizzheidi @Mr__Sniffles @winknews @DylanFedericoWX @julianamwx @sydneypersing @NicoleGabeTV I pray for everyone affected by #HurricaneIan, but not those associated with WINKnews.  No sympathy for animal abusers, liars, and those that condone it.\n","Terms after processing the text': \n","['pray', 'everyon', 'affect', 'hurrican', 'ian', 'associ', 'win', 'knew', 'sympathi', 'anim', 'abus', 'liar', 'condon']\n","Original full text of the tweet': \n","Ace Handyman Services hopes everyone was safe during the Hurricane. Any damages caused by the hurricane is our first priority! Call and schedule an appointment with one of our multi-skilled craftsmen today! üìû813-565-2022\n","#HurricaneIan #AHS #BringingHelpfulToYourHome https://t.co/BfpOq7tJE0\n","Terms after processing the text': \n","['ace', 'handyman', 'servic', 'hope', 'everyon', 'safe', 'hurrican', 'damag', 'caus', 'hurrican', 'first', 'prioriti', 'call', 'schedul', 'appoint', 'one', 'multi', 'skill', 'craftsmen', 'today', '813', '565', '2022', 'hurrican', 'ian', 'ah', 'bring', 'help', 'home']\n","Original full text of the tweet': \n","Storm surge issues in Georgetown, SC #HurricaneIan https://t.co/qWs0XJzGMx\n","Terms after processing the text': \n","['storm', 'surg', 'issu', 'georgetown', 'sc', 'hurrican', 'ian']\n","Original full text of the tweet': \n","Our thoughts are with the students, teachers, parents, and communities, suffering in the wake of Hurricane Ian. \n","\n","#CloseUpDC #HurricaneIan #Florida #Georgia #NorthCarolina #SouthCarolina https://t.co/eHZ9NKhCgA\n","Terms after processing the text': \n","['thought', 'student', 'teacher', 'parent', 'commun', 'suffer', 'wake', 'hurrican', 'ian', 'close', 'dc', 'hurrican', 'ian', 'florida', 'georgia', 'north', 'carolina', 'south', 'carolina']\n","Original full text of the tweet': \n","#SouthCarolina braces for #HurricaneIan to make landfall within HOURS https://t.co/d9sZsk0atW via @MailOnline\n","Terms after processing the text': \n","['south', 'carolina', 'brace', 'hurrican', 'ian', 'make', 'landfal', 'within', 'hour', 'via']\n","Original full text of the tweet': \n","How pissed is GOD to send #HurricaneIan to Florida and South Carolina!?\n","\n","The #MAGA cult has angered GOD and are paying for their sins.\n","\n","@RonDeSantisFL @scgovernorpress #Florida #SouthCarolina #MyrtleBeach\n","Terms after processing the text': \n","['piss', 'god', 'send', 'hurrican', 'ian', 'florida', 'south', 'carolina', 'maga', 'cult', 'anger', 'god', 'pay', 'sin', 'florida', 'south', 'carolina', 'myrtl', 'beach']\n","Original full text of the tweet': \n","Today's edition of The Smoke Eater is a photo essay on #HurricaneIan.\n","\n","Also, there are animal bleps.\n","\n","https://t.co/ZwjBziC9jf\n","Terms after processing the text': \n","['today', 'edit', 'smoke', 'eater', 'photo', 'essay', 'hurrican', 'ian', 'also', 'anim', 'blep']\n","Total time to create the index: 2.95 seconds\n"]}]},{"cell_type":"code","source":["# check the first index results for a term\n","print(\"Index results for the term 'hurricane': {}\\n\".format(index['hurricane']))\n","print(\"First 10 Index results for the term 'hurrican': \\n{}\".format(index['hurrican'][:10]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb13xRsvp2mT","executionInfo":{"status":"ok","timestamp":1667932355515,"user_tz":-60,"elapsed":13,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"fb9331b2-2c7b-4151-861d-df7b35432460"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["Index results for the term 'hurricane': []\n","\n","First 10 Index results for the term 'hurrican': \n","[['doc_1', array('I', [8])], ['doc_2', array('I', [3])], ['doc_3', array('I', [4])], ['doc_4', array('I', [10])], ['doc_5', array('I', [3])], ['doc_6', array('I', [6, 9, 23])], ['doc_7', array('I', [5])], ['doc_8', array('I', [7, 11])], ['doc_9', array('I', [3])], ['doc_10', array('I', [3])]]\n"]}]},{"cell_type":"code","source":["# check the tweets index result for a tweet\n","print(tweets_index[\"doc_4\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Op_S1BdIV0oc","executionInfo":{"status":"ok","timestamp":1667932355516,"user_tz":-60,"elapsed":10,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"028340fd-f849-4390-ce72-a8d3e22bc95e"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': 'I have this one tree in my backyard that scares me more than the poltergeist tree when it‚Äôs storming and windy like this. #scwx #HurricaneIan', 'username': 'spiralgypsy', 'date': 'Fri Sep 30 18:38:57 +0000 2022', 'hashtags': [{'text': 'scwx', 'indices': [122, 127]}, {'text': 'HurricaneIan', 'indices': [128, 141]}], 'likes': 0, 'retweets': 0}\n"]}]},{"cell_type":"markdown","source":["#----------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"4pIzRzY-ZFGq"}},{"cell_type":"markdown","source":["#**Here starts the second part of the project**"],"metadata":{"id":"vj8mIceoZJxx"}},{"cell_type":"markdown","source":["#----------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"txU7Im0eZT8J"}},{"cell_type":"markdown","source":["#**5. Build the inverted index and the tf-idf search**"],"metadata":{"id":"yl3SYJBZdD6g"}},{"cell_type":"code","source":["def create_index_tf_idf(tweets, num_tweets):\n","  \"\"\"\n","  Create the inverted index, the tweets dictionary and compute the tf, df and idf\n","\n","  Argument:\n","  tweets -- collection of tweets\n","  num_tweets -- total number of tweets\n","  \n","  Returns:\n","  index -- the inverted index. Contains the terms as keys and in which tweets (appear as the document number related to the tweet id)\n","  and in which position inside this tweet appears each term\n","  tf -- normalized term frequency for each term in each tweet\n","  df -- number of document each term appear in\n","  idf -- inverse document frequency of each term\n","  tweets-index -- the tweet's dictionary. Contains an entry for each tweet which key is the document number related with\n","  the tweet's id. Each tweet has its text, username, date, hashtags, number of likes, number of retweets and url if they exist \n","  \"\"\"\n","  index = defaultdict(list) # Create the inverted index\n","  tweets_index = defaultdict(str) # Create the tweets dictionary\n","  tf = defaultdict(list)  # Create the term frequency dictionary\n","  df = defaultdict(int)  # Create the tweet frequency dictionary\n","  idf = defaultdict(float) # Create the inverse tweet frequency dictionary\n","  \n","  for tweet in tweets:\n","    # for each tweet we create a dictionary containing the text, username, date, hashtags, number of likes, number of retweets and url if they exist\n","    tweet_dict = {}\n","    try:\n","      tweet_dict[\"text\"] = tweet[\"full_text\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"username\"] = tweet['user']['screen_name']\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"date\"] = tweet[\"created_at\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"hashtags\"] = []\n","      for i in range(0, len(tweet[\"entities\"][\"hashtags\"])):\n","        tweet_dict[\"hashtags\"].append(tweet[\"entities\"][\"hashtags\"][i])\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"likes\"] = tweet[\"favorite_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"retweets\"] = tweet[\"retweet_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"url\"] = tweet[\"entities\"][\"media\"][0][\"url\"]\n","    except:\n","      pass\n","\n","    tweets_index[doc_id[str(tweet[\"id\"])]] = tweet_dict # save the tweet in tweets index by the document number related with the tweet id\n","\n","    terms = build_terms(tweet[\"full_text\"]) # call build terms for processing the text of the tweet\n","\n","    try:\n","      tweet_dict[\"normalised_text\"] = terms\n","    except:\n","      pass\n","\n","    current_page_index = {}\n","\n","    for position, term in enumerate(terms): # loop over all terms\n","        try:\n","            # if the term is already in the index for the current page append the position\n","            current_page_index[term][1].append(position)\n","        except:\n","            # else add the new term as dict key and set the document number corresponding to this tweet and the position where the term appears in this tweet\n","            current_page_index[term]=[doc_id[str(tweet[\"id\"])], array('I',[position])] #'I' indicates unsigned int (int in Python)\n","\n","    # calculate the denominator to normalize term frequencies for not letting the size of the document matters the ranking\n","    norm = 0\n","    for term, posting in current_page_index.items():\n","        norm += len(posting[1]) ** 2\n","    norm = math.sqrt(norm)\n","\n","    # compute the tf (so the frequency of a term in the document) and the df (number of documents containing a certain term)\n","    for term, posting in current_page_index.items():\n","        tf[term].append(np.round(len(posting[1])/norm,4))\n","        df[term] += 1\n","    \n","    # compute the idf (for giving more importance to the terms that appear in less documents)\n","    for term in df:\n","        idf[term] = np.round(np.log(float(num_tweets/df[term])), 4)\n","\n","    # merge the current page index with the main index\n","    for term_page, posting_page in current_page_index.items():\n","        index[term_page].append(posting_page)\n","\n","  return index, tf, df, idf, tweets_index"],"metadata":{"id":"0S0-nRNVdN4s","executionInfo":{"status":"ok","timestamp":1667932355516,"user_tz":-60,"elapsed":9,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["start_time = time.time() # we save the current time for calculating how much time will take to run our create_index_tfidf() function\n","num_tweets = len(tweets)\n","index, tf, df, idf, tweets_index = create_index_tf_idf(tweets, num_tweets) # we call the function\n","print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2))) # we calculate the current time less the start time for getting the total time that our function lasted"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0puh95w3djBR","executionInfo":{"status":"ok","timestamp":1667932557257,"user_tz":-60,"elapsed":201748,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"ba238cea-aa1f-4599-8b20-a76a97ac5ccb"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Total time to create the index: 201.57 seconds\n"]}]},{"cell_type":"code","source":["def rank_documents(terms, docs, index, idf, tf, tweets_index):\n","    \"\"\"\n","    Rank the results of a query based on the tf-idf weights that we have previously calculated\n","    \n","    Argument:\n","    terms -- list of terms\n","    tweets -- list of tweets to rank matching the query\n","    index -- inverted index\n","    idf -- inverted document frequencies\n","    tf -- term frequencies\n","    tweets_index -- mapping between document number and tweet id\n","    \n","    Returns:\n","    result_docs -- list in order of ranked documents\n","    \"\"\"\n","\n","    # We are only interested on the elements of the docVector corresponding to the query term, due to this, the others will become 0 once that they are multiplied by the query_vector\n","    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n","    query_vector = [0] * len(terms)\n","\n","    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n","\n","    query_norm = la.norm(list(query_terms_count.values())) # compute the norm for the query tf\n","\n","    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n","        if term not in index:\n","            continue\n","\n","        ## Compute tf*idf\n","        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n","\n","        # Generate doc_vectors for matching docs\n","        for doc_index, (doc, postings) in enumerate(index[term]):      \n","            if doc in docs: \n","                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n","\n","    # Calculate the score of each doc \n","    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n","    doc_scores.sort(reverse=True) # sort the doc_scores\n","    \n","    result_docs = [x[1] for x in doc_scores]\n","\n","    # print no results found if no results are found, ask to write another query and call again the search_tf_idf() function\n","    if len(result_docs) == 0:\n","        print(\"No results found, try again\")\n","        query = input()\n","        docs = search_tf_idf(query, index)\n","    #print ('\\n'.join(result_docs), '\\n')\n","    return result_docs"],"metadata":{"id":"6-dKYUhidl1A","executionInfo":{"status":"ok","timestamp":1667932557258,"user_tz":-60,"elapsed":32,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["def search_tf_idf(query, index):\n","    \"\"\"\n","    Obtain the list of ranked documents based on the query\n","    \n","    Argument:\n","    query -- list of terms that we want to search\n","    index -- inverted index\n","    \n","    Returns:\n","    ranked_docs -- list in order of ranked documents\n","    \"\"\"\n","    query = build_terms(query) # normalize the query\n","    docs = set() # create an empty set where we will store the ordered docs for each word in the query\n","    counter = 0 # create a counter for distinguishing between the first term and the rest of them\n","    for term in query:\n","        try:\n","            # store in term_docs the ids of the docs that contain the current term                    \n","            term_docs=[posting[0] for posting in index[term]]\n","            \n","            # keep the documents that have all terms of the query\n","            if counter == 0:\n","              # we have to distinguish for the first term, because otherwise, the intersection between a empty set and another set is always an empty set\n","              docs = docs.union(term_docs)\n","              counter += 1\n","            else:\n","              docs = docs.intersection(term_docs)\n","        except:\n","            # pass if the term is not in the index\n","            pass\n","    docs = list(docs)\n","    # rank only the docs that keep in docs, so only the docs that contain all the terms in the query\n","    ranked_docs = rank_documents(query, docs, index, idf, tf, tweets_index)\n","    return ranked_docs # return the ranked docs"],"metadata":{"id":"8UEiDdIhdoK-","executionInfo":{"status":"ok","timestamp":1667932557259,"user_tz":-60,"elapsed":31,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["#**6. Select the queries to use**"],"metadata":{"id":"g_7M2XCOfQPd"}},{"cell_type":"code","source":["maximum = 0\n","maximum_2 = 0\n","maximum_3 = 0\n","maximum_4 = 0\n","max_word_tf = \"\"\n","second_max_word_tf = \"\"\n","third_max_word_tf = \"\"\n","fourth_max_word_tf = \"\"\n","# find the four documents with higher tf\n","for i in tf:\n","  if i != \"hurricaneian\": # do not have into account the term hurricaneian\n","    if sum(tf[i]) > maximum:\n","      maximum_4 = maximum_3\n","      fourth_max_word_tf = third_max_word_tf\n","      maximum_3 = maximum_2\n","      third_max_word_tf = second_max_word_tf\n","      maximum_2 = maximum\n","      second_max_word_tf = max_word_tf\n","      maximum = sum(tf[i])\n","      max_word_tf = i\n","\n","    elif sum(tf[i]) > maximum_2:\n","      maximum_4 = maximum_3\n","      fourth_max_word_tf = third_max_word_tf\n","      maximum_3 = maximum_2\n","      third_max_word_tf = second_max_word_tf\n","      maximum_2 = sum(tf[i])\n","      second_max_word_tf = i\n","\n","    elif (sum(tf[i]) > maximum_3):\n","      maximum_4 = maximum_3\n","      fourth_max_word_tf = third_max_word_tf\n","      maximum_3 = sum(tf[i])\n","      third_max_word_tf = i\n","\n","    elif (sum(tf[i]) > maximum_4):\n","      maximum_4 = sum(tf[i])\n","      fourth_max_word_tf = i\n","\n","# print the results\n","print(max_word_tf)\n","print(second_max_word_tf)\n","print(third_max_word_tf)\n","print(fourth_max_word_tf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_z_c90c7fUIY","executionInfo":{"status":"ok","timestamp":1667932557259,"user_tz":-60,"elapsed":30,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"7ea7adae-5469-4b23-8e81-8f75548ff23f"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["hurrican\n","ian\n","florida\n","carolina\n"]}]},{"cell_type":"markdown","source":["#**7. Print the ranked documents for a given query**"],"metadata":{"id":"Y6lzzLHrklR9"}},{"cell_type":"code","source":["query = max_word_tf + \" \" + second_max_word_tf\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results for the two highest tf documents\n","print(\"\\n======================\\nTop {} results out of {} for the searched query {}:\\n\".format(top, len(ranked_docs), query))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eR0nsK-MhasL","executionInfo":{"status":"ok","timestamp":1667932557773,"user_tz":-60,"elapsed":538,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"dd130540-1233-403f-ebc2-ece8f59e9928"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================\n","Top 10 results out of 3655 for the searched query hurrican ian:\n","\n","doc_id= doc_985 - tweet_text: Is that you #HurricaneIan? https://t.co/5K6V610Ivf\n","doc_id= doc_640 - tweet_text: Hurricane Ian before and after #HurricaneIan https://t.co/XZstkI2pN2\n","doc_id= doc_2597 - tweet_text: That's how it is üòÇ\n","#HurricaneIan üåÄ https://t.co/9BSWTb2Bwf\n","doc_id= doc_1992 - tweet_text: Before + After. #HurricaneIan https://t.co/kYIGeNTeD2\n","doc_id= doc_634 - tweet_text: Hurricane IAN #Ian #HurricaneIan #HurricanIan #Huracan #HuracanIan #Hurricane https://t.co/HbllO4Q3vB\n","doc_id= doc_1217 - tweet_text: Hurricane Ian on tourüò≠\n","#HurricaneIan\n","doc_id= doc_920 - tweet_text: ü§Øüí•Touring Hurricane Ian's 'ground zero'\n","\n","https://t.co/1wCsrypPqr\n","\n","#HurricaneIan #IanHurricane #HurricanIan #HurricaneFiona #HelloJimtober #Florida #miami #Cuba #KeyWest  #Ian\n","doc_id= doc_894 - tweet_text: ü§Øüí•Touring Hurricane Ian's 'ground zero'\n","\n","https://t.co/Xlb4pP9DSt\n","\n","#HurricaneIan #IanHurricane #HurricanIan #HurricaneFiona #HelloJimtober #Florida #miami #Cuba #KeyWest  #Ian\n","doc_id= doc_775 - tweet_text: üå™Ô∏èüí•FEMA's responds to Hurricane Ian's devastation in Florida\n","\n","https://t.co/zW4iZTsM4W\n","\n","#HurricaneIan #IanHurricane #HurricanIan #HurricaneFiona #HelloJimtober #Florida #miami #Cuba #KeyWest  #Ian\n","doc_id= doc_705 - tweet_text: üå™Ô∏èIt's here! \n","\n","WATCHüì∫Myrtle Beach live camera | Hurricane Ian approaches\n","\n","https://t.co/2KBmY0dYt4\n","\n","#HurricaneIan #IanHurricane #HurricanIan #HurricaneFiona #HelloJimtober #Florida #miami #Cuba #KeyWest  #Ian\n"]}]},{"cell_type":"code","source":["query = max_word_tf + \" \" + third_max_word_tf\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results for the two highest idf documents\n","print(\"\\n======================\\nTop {} results out of {} for the searched query {}:\\n\".format(top, len(ranked_docs), query))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBVwHKybhr04","executionInfo":{"status":"ok","timestamp":1667932558332,"user_tz":-60,"elapsed":575,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"fe5f1881-7fb3-413f-ed0a-4cefce2bb05f"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================\n","Top 10 results out of 892 for the searched query hurrican florida:\n","\n","doc_id= doc_3949 - tweet_text: My thoughts üí≠üôèüèªüôèüèªüôèüèª with #Florida after the Hugs #HurricaneIan ... #PrayingForFlorida https://t.co/8wwPQzZ207\n","doc_id= doc_3296 - tweet_text: This Florida flamboyance of flamingos is safe!\n","\n","https://t.co/puQNCkDooz\n","\n","#HurricaneIan #Florida\n","doc_id= doc_3944 - tweet_text: No force can be stronger than force of nature #HurricaneIan \n","Prayers for South-west Florida!\n","#Florida \n","#FloridaStrong \n","@FLGuard @ActionNewsJax @FoxNews @wjxt4 @cnnbrk @BBCWorld \n","@ManuGulati11 @IamCJha @BhavnaSawnani https://t.co/s2vEjBW6WI\n","doc_id= doc_1615 - tweet_text: Praying for everyone down in my community in Florida. #Naples #Florida #HurricaneIan\n","doc_id= doc_111 - tweet_text: Florida‚Äôs Long Road To Recovery\n","#florida #hurricaneian #hurricane #ian \n","https://t.co/zcE3Yv79bg\n","doc_id= doc_2937 - tweet_text: If you‚Äôre having issues contacting your family in Florida and are worried go to https://t.co/O0otTDdBlD¬†#Florida #HurricaneIan #ContactingFamily #MissingFlorida @10TampaBay\n","doc_id= doc_2992 - tweet_text: Praying for everyone in the Carolina‚Äôs now! Florida‚Ä¶how we holding up? Daytona Beach still has no power üòî\n","\n","#HurricaneIan \n","#Ian2022 \n","#Florida\n","#FloridaStrongüí™üèæ\n","doc_id= doc_3910 - tweet_text: Thinking about #Florida right nowü§ç\n","#FloridaStrong #FortMyers #HurricaneIan\n","doc_id= doc_2999 - tweet_text: If anyone out there wants to donate to the Florida Disaster Fund to help with the hurricane recovery - https://t.co/tBvl2d3mew #ServeFL #hurricaneian #Hurricane_Ian #floridahurricane #FloridaStrong #FloridaStorm\n","doc_id= doc_413 - tweet_text: Keep voting for #climatechangedeniers you Florida idiots. #HurricaneIan #Florida #DeSantisDestroysFlorida No mercy for Trumptards and Fox News watchers. Maybe losing your house will wake you the F up\n"]}]},{"cell_type":"code","source":["query = max_word_tf + \" \" + fourth_max_word_tf\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results for the highest tf document and the highest idf document\n","print(\"\\n======================\\nTop {} results out of {} for the searched query {}:\\n\".format(top, len(ranked_docs), query))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoxnSGT5hv6z","executionInfo":{"status":"ok","timestamp":1667932558332,"user_tz":-60,"elapsed":48,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"9e252ec7-5301-4397-c969-5083b57d92ba"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================\n","Top 10 results out of 412 for the searched query hurrican carolina:\n","\n","doc_id= doc_1335 - tweet_text: WATCH: After ravaging Florida, a strengthened Hurricane Ian has made landfall in South Carolina and is predicted to enter North Carolina soon.  #Florida #NorthCarolina #SouthCarolina #USA #HurricaneIan #Environment https://t.co/LIAALDsxeP\n","doc_id= doc_1444 - tweet_text: #HurricaneIan nears landfall in South Carolina - Will impact both Carolinas and Virginia, reports Karen Graham. https://t.co/qttgs1Stbf\n","doc_id= doc_3343 - tweet_text: Hello #Ian... we've been expecting you. #HurricaneIan #SouthCarolina #MurrellsInlet #CoastalCarolina https://t.co/gs6iRw8vmD\n","doc_id= doc_701 - tweet_text: Hunker Down, #Carolinas #HurricaneIan.\n","doc_id= doc_254 - tweet_text: South Carolina #HurricaneIan https://t.co/yTA4dFUC2V\n","doc_id= doc_2452 - tweet_text: Watch:  Oak Island now #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/mHIM1UpYbD\n","doc_id= doc_648 - tweet_text: Hurricane Ian \n","Category 1\n","16 Miles SE of Georgetown South Carolina \n","Wind Speed - 85 mph; Wind Gusts - 105mph \n","Movement - N @ 15 mph\n","NWS- Sandhill region of North Carolina under a Tornado Watch until 1000pm Friday. Heavy rain, winds.\n","#HurricaneIan #NorthCarolina #SouthCarolina\n","doc_id= doc_2513 - tweet_text: Watch:  Not quite sure where exactly..... #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/h0srna3zY8\n","doc_id= doc_2284 - tweet_text: Watch:  Not quite sure where exactly..... #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/cpTAAYAhYN\n","doc_id= doc_957 - tweet_text: #Ian #HurricaneIan #SC #NC #VA #TornadoWatch #SouthCarolina #NorthCarolina #Virginia\n","\n","From ‚Å¶@accuweather‚Å© channel. https://t.co/sTipeTg9Wq\n"]}]},{"cell_type":"code","source":["query = second_max_word_tf + \" \" + third_max_word_tf\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results for the second highest tf document and the second highest idf document\n","print(\"\\n======================\\nTop {} results out of {} for the searched query {}:\\n\".format(top, len(ranked_docs), query))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhUb1F6Whwex","executionInfo":{"status":"ok","timestamp":1667932558333,"user_tz":-60,"elapsed":36,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"750d4fff-2820-4e30-8e25-9e8788bbac4e"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================\n","Top 10 results out of 886 for the searched query ian florida:\n","\n","doc_id= doc_3949 - tweet_text: My thoughts üí≠üôèüèªüôèüèªüôèüèª with #Florida after the Hugs #HurricaneIan ... #PrayingForFlorida https://t.co/8wwPQzZ207\n","doc_id= doc_3296 - tweet_text: This Florida flamboyance of flamingos is safe!\n","\n","https://t.co/puQNCkDooz\n","\n","#HurricaneIan #Florida\n","doc_id= doc_3944 - tweet_text: No force can be stronger than force of nature #HurricaneIan \n","Prayers for South-west Florida!\n","#Florida \n","#FloridaStrong \n","@FLGuard @ActionNewsJax @FoxNews @wjxt4 @cnnbrk @BBCWorld \n","@ManuGulati11 @IamCJha @BhavnaSawnani https://t.co/s2vEjBW6WI\n","doc_id= doc_1615 - tweet_text: Praying for everyone down in my community in Florida. #Naples #Florida #HurricaneIan\n","doc_id= doc_111 - tweet_text: Florida‚Äôs Long Road To Recovery\n","#florida #hurricaneian #hurricane #ian \n","https://t.co/zcE3Yv79bg\n","doc_id= doc_2937 - tweet_text: If you‚Äôre having issues contacting your family in Florida and are worried go to https://t.co/O0otTDdBlD¬†#Florida #HurricaneIan #ContactingFamily #MissingFlorida @10TampaBay\n","doc_id= doc_2992 - tweet_text: Praying for everyone in the Carolina‚Äôs now! Florida‚Ä¶how we holding up? Daytona Beach still has no power üòî\n","\n","#HurricaneIan \n","#Ian2022 \n","#Florida\n","#FloridaStrongüí™üèæ\n","doc_id= doc_3910 - tweet_text: Thinking about #Florida right nowü§ç\n","#FloridaStrong #FortMyers #HurricaneIan\n","doc_id= doc_2999 - tweet_text: If anyone out there wants to donate to the Florida Disaster Fund to help with the hurricane recovery - https://t.co/tBvl2d3mew #ServeFL #hurricaneian #Hurricane_Ian #floridahurricane #FloridaStrong #FloridaStorm\n","doc_id= doc_413 - tweet_text: Keep voting for #climatechangedeniers you Florida idiots. #HurricaneIan #Florida #DeSantisDestroysFlorida No mercy for Trumptards and Fox News watchers. Maybe losing your house will wake you the F up\n"]}]},{"cell_type":"code","source":["query = second_max_word_tf + \" \" + fourth_max_word_tf\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results for the highest tf document and the second highest idf document\n","print(\"\\n======================\\nTop {} results out of {} for the searched query {}:\\n\".format(top, len(ranked_docs), query))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"So5PV0BShwug","executionInfo":{"status":"ok","timestamp":1667932558333,"user_tz":-60,"elapsed":23,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"f8a92d0e-5fb5-4fc4-da95-221ac47b5e9f"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================\n","Top 10 results out of 409 for the searched query ian carolina:\n","\n","doc_id= doc_1335 - tweet_text: WATCH: After ravaging Florida, a strengthened Hurricane Ian has made landfall in South Carolina and is predicted to enter North Carolina soon.  #Florida #NorthCarolina #SouthCarolina #USA #HurricaneIan #Environment https://t.co/LIAALDsxeP\n","doc_id= doc_1444 - tweet_text: #HurricaneIan nears landfall in South Carolina - Will impact both Carolinas and Virginia, reports Karen Graham. https://t.co/qttgs1Stbf\n","doc_id= doc_3343 - tweet_text: Hello #Ian... we've been expecting you. #HurricaneIan #SouthCarolina #MurrellsInlet #CoastalCarolina https://t.co/gs6iRw8vmD\n","doc_id= doc_701 - tweet_text: Hunker Down, #Carolinas #HurricaneIan.\n","doc_id= doc_254 - tweet_text: South Carolina #HurricaneIan https://t.co/yTA4dFUC2V\n","doc_id= doc_2452 - tweet_text: Watch:  Oak Island now #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/mHIM1UpYbD\n","doc_id= doc_648 - tweet_text: Hurricane Ian \n","Category 1\n","16 Miles SE of Georgetown South Carolina \n","Wind Speed - 85 mph; Wind Gusts - 105mph \n","Movement - N @ 15 mph\n","NWS- Sandhill region of North Carolina under a Tornado Watch until 1000pm Friday. Heavy rain, winds.\n","#HurricaneIan #NorthCarolina #SouthCarolina\n","doc_id= doc_2513 - tweet_text: Watch:  Not quite sure where exactly..... #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/h0srna3zY8\n","doc_id= doc_2284 - tweet_text: Watch:  Not quite sure where exactly..... #HurricaneIan #scwx #ncwx #ian #NorthCarolina #SouthCarolina https://t.co/cpTAAYAhYN\n","doc_id= doc_957 - tweet_text: #Ian #HurricaneIan #SC #NC #VA #TornadoWatch #SouthCarolina #NorthCarolina #Virginia\n","\n","From ‚Å¶@accuweather‚Å© channel. https://t.co/sTipeTg9Wq\n"]}]},{"cell_type":"code","source":["# Here, we let you write your own querie just in case you want to check if our ranking system works for any query\n","print(\"Insert your query (i.e.: Computer Science):\\n\")\n","query = input() # we ask to the user the query that they want to search\n","ranked_docs = search_tf_idf(query, index) # we call search_tf_idf() for obtaining the ranked documents containing all the terms in that query\n","top = 10\n","\n","# We print the top 10 results\n","print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n","for d_id in ranked_docs[:top]:\n","    print(\"doc_id= {} - tweet_text: {}\".format(d_id, tweets_index[d_id][\"text\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebjlrZWQdqwN","executionInfo":{"status":"ok","timestamp":1667932564947,"user_tz":-60,"elapsed":6628,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"f77b272d-7535-4034-936e-805f6f7d5359"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Insert your query (i.e.: Computer Science):\n","\n","hurrican\n","\n","======================\n","Top 10 results out of 3704 for the searched query:\n","\n","doc_id= doc_2267 - tweet_text: Hurricane Q&amp;A with Kent A. Showalter III, Esq. \n","\n","#HurricaneIan #HurricaneSeason #HurricaneInfo #Hurricane2022 #HurricaneOrlando #HurricaneFortMeyers #HurricaneRecoverySupport #HurricaneRecovery #HurricaneAssistance #DSKLAWGroup #HurricaneTips https://t.co/nJykX4Sgxx\n","doc_id= doc_2263 - tweet_text: Hurricane Q&amp;A with Kent A. Showalter III, Esq. \n","\n","#HurricaneIan #HurricaneSeason #HurricaneInfo #Hurricane2022 #HurricaneOrlando #HurricaneFortMeyers #HurricaneRecoverySupport #HurricaneRecovery #HurricaneAssistance #DSKLAWGroup #HurricaneTips https://t.co/urQ15eIGmv\n","doc_id= doc_1986 - tweet_text: Stay home!\n"," #DoveHurricaneGuide #HurricaneIan #Hurricane_Ian #HurricaneSeason #Hurricanes https://t.co/7XLjUNmTt8\n","doc_id= doc_628 - tweet_text: \" I Survived Hurricane Ian\" \n","Visit  #redbubbleshop for this design on clothing &amp; home decor.\n","GO TO:  https://t.co/5xe06qXU55\n","||| #HurricaneIan #HurricaneIanupdate #Hurricane_Ian #Hurricane #hurricanes #HurricaneSeason #floridahurricane #FloridaStorm #orlandofl #Tampa #FortMeyers https://t.co/GLMo3C4VSH\n","doc_id= doc_619 - tweet_text: It is much better to be over-prepared for a hurricane than under-prepared.\n","\n","#HurricaneIan #Hurricane #HurricanIan\n","doc_id= doc_3725 - tweet_text: Know your hurricane cats. #HurricaneIan https://t.co/9q3XC3IYVd\n","doc_id= doc_2537 - tweet_text: PRAY üôè FOR ALL WHO SUFFERED FROM THE HURRICANE #HurricaneIan\n","doc_id= doc_2188 - tweet_text: Hurricanes are no joke. \n","#HurricaneIan \n","https://t.co/atrHw0KDyC 01\n","doc_id= doc_499 - tweet_text: Jim Cantore hanging on to a street sign to keep from blowing away is some damn masculine energy. We believe you, Jim! There‚Äôs a hurricane! Get the hell inside! #jimcantore #TheWeatherChannel #jimatthegym #Cantore #HurricaneIan #Hurricane_Ian #Hurricane #HurricaneSeason #hurricane\n","doc_id= doc_2603 - tweet_text: \"#Hurricane season brings a humbling reminder that, despite our technologies, most of nature remains unpredictable.\" üåÄDiane Ackerman\n","\n","List of #PuertoRico hurricanes üáµüá∑ Wikipedia \n","\n","https://t.co/RCrHVgGbXt\n","#weather #DYK\n","#HurricaneMaria #HurricaneIan\n","#HurricanFiona #climatechange https://t.co/NBjzKHt2by\n"]}]},{"cell_type":"markdown","source":["#**8. Load the datasets 2**"],"metadata":{"id":"UHrLPkO1jHIO"}},{"cell_type":"code","source":["# load the new dataset for this lab using pandas\n","evaluation_gt = pd.read_csv(\"drive/MyDrive/IRWA/Part_2:Indexing_and_Evaluation/evaluation_gt.csv\")\n","evaluation_gt.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"z1v_78R1lLzp","executionInfo":{"status":"ok","timestamp":1667932564948,"user_tz":-60,"elapsed":19,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"799053ff-bdcc-4762-d6cb-2385758fc8a0"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       doc  query_id  label\n","0   doc_12         1      1\n","1    doc_9         1      1\n","2   doc_18         1      1\n","3   doc_45         1      1\n","4  doc_501         1      1"],"text/html":["\n","  <div id=\"df-8d328aac-122e-4cf6-9b8b-8d295a43901a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc</th>\n","      <th>query_id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>doc_12</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>doc_9</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>doc_18</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>doc_45</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>doc_501</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d328aac-122e-4cf6-9b8b-8d295a43901a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8d328aac-122e-4cf6-9b8b-8d295a43901a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8d328aac-122e-4cf6-9b8b-8d295a43901a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","source":["#**9. Do the ranking**"],"metadata":{"id":"n08kxk23wzLK"}},{"cell_type":"code","source":["def rank_documents_2(terms, docs, index, idf, tf, tweets_index):\n","    \"\"\"\n","    Rank the results of a query based on the tf-idf weights that we have previously calculated\n","    \n","    Argument:\n","    terms -- list of terms\n","    tweets -- list of tweets to rank matching the query\n","    index -- inverted index\n","    idf -- inverted document frequencies\n","    tf -- term frequencies\n","    tweets_index -- mapping between document number and tweet id\n","    \n","    Returns:\n","    doc_scores -- tf-idf score of each document\n","    \"\"\"\n","\n","    # We do once again the same as in rank_documents() function but finishing the function a bit earlier, before ordering the documents, as now we are interested in the scores:\n","\n","    # We are only interested on the elements of the docVector corresponding to the query term, due to this, the others will become 0 once that they are multiplied by the query_vector\n","    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n","    query_vector = [0] * len(terms)\n","\n","    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n","\n","    query_norm = la.norm(list(query_terms_count.values())) # compute the norm for the query tf\n","\n","    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n","        if term not in index:\n","            continue\n","\n","        ## Compute tf*idf\n","        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term]\n","\n","        # Generate doc_vectors for matching docs\n","        for doc_index, (doc, postings) in enumerate(index[term]):      \n","            if doc in docs: \n","                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n","\n","    # Calculate the score of each doc \n","    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n","\n","    return doc_scores"],"metadata":{"id":"_-5iSPV_wyhj","executionInfo":{"status":"ok","timestamp":1667932564949,"user_tz":-60,"elapsed":15,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["# copy the queries\n","queries = [\"Landfall in South Carolina\", \"Help and recovery during the hurricane disaster\", \"Floodings in South Carolina\"]\n","# separate in dataframes evaluation_gt by the query_id\n","ev_gt = [evaluation_gt[evaluation_gt[\"query_id\"] == 1], evaluation_gt[evaluation_gt[\"query_id\"] == 2], evaluation_gt[evaluation_gt[\"query_id\"] == 3]]\n","\n","# apply the pre-processing techniques to our queries \n","for q in range(0, len(queries)):\n","  queries[q] = build_terms(queries[q])\n","\n","# merge the ranking results obtained through rank_documents_2 with the correspondent ev_gt\n","for q in range(0, len(queries)):\n","  docs_ev_gt = ev_gt[q].iloc[:,0].values\n","  ranked_docs = rank_documents_2(queries[q], docs_ev_gt, index, idf, tf, tweets_index)\n","  df_ranked_docs = pd.DataFrame()\n","  counter = 0\n","  for i in ranked_docs:\n","    df_ranked_docs.loc[counter, \"score\"] = i[0]\n","    df_ranked_docs.loc[counter, \"doc\"] = i[1]\n","    counter += 1\n","  ev_gt[q] = ev_gt[q].merge(df_ranked_docs, on=\"doc\", how='left')\n","  ev_gt[q]['score'] = ev_gt[q]['score'].fillna(0)"],"metadata":{"id":"_Naf6KuJep8U","executionInfo":{"status":"ok","timestamp":1667932564950,"user_tz":-60,"elapsed":15,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["our_queries = [max_word_tf + \" \" + second_max_word_tf, max_word_tf + \" \" + third_max_word_tf, max_word_tf + \" \" + fourth_max_word_tf, second_max_word_tf + \" \" + third_max_word_tf, second_max_word_tf + \" \" + fourth_max_word_tf]\n","\n","# pre-process our queries just in case\n","for q in range(0, len(our_queries)):\n","  our_queries[q] = build_terms(our_queries[q])\n","\n","# create a set containing the first 20 tweets\n","docs_2 = []\n","for i in range(0, 21):\n","  docs_2.append(\"doc_\"+str(i))\n","\n","# merge the ranking results obtained through rank_documents_2 with the correspondent ev_gt\n","ev_gt_our_q = []\n","for q in range(0, len(our_queries)):\n","  ranked_docs = rank_documents_2(our_queries[q], docs_2, index, idf, tf, tweets_index)\n","  df_ranked_docs = pd.DataFrame()\n","  counter = 0\n","  for i in ranked_docs:\n","    df_ranked_docs.loc[counter, \"doc\"] = i[1]\n","    df_ranked_docs.loc[counter, \"query_id\"] = q\n","    df_ranked_docs.loc[counter, \"score\"] = i[0]\n","    counter += 1\n","  df_ranked_docs['score'] = df_ranked_docs['score'].fillna(0)\n","  ev_gt_our_q.append(df_ranked_docs)\n","\n","# Collect the docs having both terms of the queries\n","docs_arr_our_queries = []\n","for q in our_queries:\n","  docs = set()\n","  counter = 0\n","  for term in q:\n","    try:\n","        # store in term_docs the ids of the docs that contain the current term                    \n","        term_docs=[posting[0] for posting in index[term]]\n","        \n","        # keep the documents that are in the term docs of all the words that we are searching in the query\n","        if counter == 0:\n","          docs = docs.union(term_docs)\n","          counter += 1\n","        else:\n","          docs = docs.intersection(term_docs)\n","    except:\n","        # pass if the term is not in the index\n","        pass\n","  docs_arr_our_queries.append(docs)\n","\n","# check if the document has both terms of the query and if so, set the label to 1\n","for q in range(0, len(docs_arr_our_queries)):\n","  for i in docs_arr_our_queries[q]:\n","    if i in docs_2:\n","      index_ = ev_gt_our_q[q][ev_gt_our_q[q][\"doc\"] == i].index\n","      ev_gt_our_q[q].loc[index_[0], \"label\"] = 1\n","\n","  ev_gt_our_q[q]['label'] = ev_gt_our_q[q]['label'].fillna(0) # set the label to 0 for the other elements\n","\n","# eliminate some tweets which seem irrelevant based on our perception\n","for i in range(0, len(our_queries)):\n","  ev_gt_our_q[i].loc[3, \"label\"] = 0\n","  ev_gt_our_q[i].loc[10, \"label\"] = 0\n","  ev_gt_our_q[i].loc[17, \"label\"] = 0 # We eliminate this one not because it is not relevant, but because its label is 1 for all our queries, and by changing this, we will \n","                                      # obtain less 1 for average precision, mrr and ndcg"],"metadata":{"id":"6o8Sy3wgt8ZG","executionInfo":{"status":"ok","timestamp":1667932565359,"user_tz":-60,"elapsed":424,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":["#**10. Precision@K and Recall@K**"],"metadata":{"id":"Gd0re0UTxBwn"}},{"cell_type":"code","source":["# Collect the docs having at least one term of the queries\n","docs_arr = []\n","for q in queries:\n","  docs = set()\n","  counter = 0\n","  for term in q:\n","    try:\n","        # store in term_docs the ids of the docs that contain the current term                    \n","        term_docs=[posting[0] for posting in index[term]]\n","        \n","         # keep the documents that are in the term docs of all the words that we are searching in the query\n","        if counter == 0:\n","          docs = docs.union(term_docs)\n","          counter += 1\n","        else:\n","          docs = docs.intersection(term_docs)\n","    except:\n","        # pass if the term is not in the index\n","        pass\n","  docs_arr.append(docs)"],"metadata":{"id":"s919gFD9Ap1Z","executionInfo":{"status":"ok","timestamp":1667932565359,"user_tz":-60,"elapsed":36,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["def precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    precision @k : float\n","\n","    \"\"\"\n","    order = np.argsort(y_score)[::-1] # get the ranking of the documents according to the predicted score\n","    doc_score = np.take(doc_score, order[:k]) # align the binary relevance to the corresponding document\n","    relevant = sum(doc_score == 1) # get number of relevant documents\n","    return float(relevant) / k # calculate precision at k"],"metadata":{"id":"lloDsJEEIGqA","executionInfo":{"status":"ok","timestamp":1667932565360,"user_tz":-60,"elapsed":36,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["# for each query\n","for q in range(0, len(queries)):\n","  k = len(ev_gt[q])\n","  print(\"Query\", q + 1, \":\") # print the query number\n","  print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(ev_gt[q][\"label\"], ev_gt[q][\"score\"], k))) # the precision\n","  print(\"==> Recall@{}: {}\\n\".format(len(docs_arr[q]), len(ev_gt[q][ev_gt[q][\"label\"] == 1])/len(docs_arr[q]))) # the recall using the doc_arr calculated above"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBIVoWLB9cb4","executionInfo":{"status":"ok","timestamp":1667932565360,"user_tz":-60,"elapsed":36,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"f41ba69e-02f6-4e96-c517-e0e2a539d72f"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["Query 1 :\n","==> Precision@20: 0.5\n","\n","==> Recall@101: 0.09900990099009901\n","\n","Query 2 :\n","==> Precision@20: 0.5\n","\n","==> Recall@8: 1.25\n","\n","Query 3 :\n","==> Precision@20: 0.5\n","\n","==> Recall@27: 0.37037037037037035\n","\n"]}]},{"cell_type":"code","source":["# repeat the precision@k and recall@k calculations for our queries\n","# repet the same for our queries\n","for q in range(0, len(our_queries)):\n","  k = len(ev_gt_our_q[q])\n","  print(\"Our Query\", q + 1, \":\")\n","  print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(ev_gt_our_q[q][\"label\"], ev_gt_our_q[q][\"score\"], k)))\n","  print(\"==> Recall@{}: {}\\n\".format(len(docs_arr_our_queries[q]), len(ev_gt_our_q[q][ev_gt_our_q[q][\"label\"] == 1])/len(docs_arr_our_queries[q])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyR6-25Uxjvy","executionInfo":{"status":"ok","timestamp":1667932565362,"user_tz":-60,"elapsed":36,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"ba77a667-d81f-4421-acab-886de01f7a3c"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Query 1 :\n","==> Precision@20: 0.85\n","\n","==> Recall@3655: 0.004651162790697674\n","\n","Our Query 2 :\n","==> Precision@20: 0.15\n","\n","==> Recall@892: 0.0033632286995515697\n","\n","Our Query 3 :\n","==> Precision@20: 0.25\n","\n","==> Recall@412: 0.012135922330097087\n","\n","Our Query 4 :\n","==> Precision@20: 0.15\n","\n","==> Recall@886: 0.003386004514672686\n","\n","Our Query 5 :\n","==> Precision@20: 0.25\n","\n","==> Recall@409: 0.012224938875305624\n","\n"]}]},{"cell_type":"markdown","source":["#**11. Average Precision@K**"],"metadata":{"id":"bpDL5aVexM40"}},{"cell_type":"code","source":["def avg_precision_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    average precision @k : float\n","    \"\"\"\n","    gtp = np.sum(doc_score == 1) # total number of gt positives\n","    order = np.argsort(y_score)[::-1] # same as for precision\n","    doc_score = np.take(doc_score, order[:k]) # same as for precision\n","    ## if all documents are not relevant\n","    if gtp == 0:\n","        return 0\n","    n_relevant_at_i = 0\n","    prec_at_i = 0\n","    for i in range(len(doc_score)):\n","        if doc_score[i] == 1: # only add the P@k when the doc is relevant\n","            n_relevant_at_i += 1\n","            prec_at_i += n_relevant_at_i / (i + 1) # calculate P@K (#docs relevant at k/k)\n","    return prec_at_i / gtp # return ap"],"metadata":{"id":"vry3mPrKxgHh","executionInfo":{"status":"ok","timestamp":1667932565363,"user_tz":-60,"elapsed":32,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["# for each query\n","for q in range(0, len(queries)):\n","  print(\"Query\", q + 1, \":\") # print the query number\n","  print(\"==> Average Precission@{}: {}\\n\".format(len(ev_gt[q]), avg_precision_at_k(np.array(ev_gt[q][\"label\"]), ev_gt[q][\"score\"], len(ev_gt[q])))) # print the average precision@k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-UEFgdYDTeC","executionInfo":{"status":"ok","timestamp":1667932565363,"user_tz":-60,"elapsed":31,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"a96a69e0-bef5-4ea9-91a3-21f5809be2c9"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["Query 1 :\n","==> Average Precission@20: 1.0\n","\n","Query 2 :\n","==> Average Precission@20: 0.9263455988455987\n","\n","Query 3 :\n","==> Average Precission@20: 1.0\n","\n"]}]},{"cell_type":"code","source":["# repeat the average precision@k calculations for our queries\n","# for each query\n","for q in range(0, len(our_queries)):\n","  print(\"Our Query\", q + 1, \":\") # print the query number\n","  print(\"==> Average Precision@{}: {}\\n\".format(len(ev_gt_our_q[q]), avg_precision_at_k(np.array(ev_gt_our_q[q][\"label\"]), ev_gt_our_q[q][\"score\"], len(ev_gt_our_q[q])))) # print the average precision@k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwRn8-4E137y","executionInfo":{"status":"ok","timestamp":1667932565364,"user_tz":-60,"elapsed":29,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"8d59a418-4d4f-4d0b-d2fb-0abe86538a15"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Query 1 :\n","==> Average Precision@20: 0.8065874666926389\n","\n","Our Query 2 :\n","==> Average Precision@20: 1.0\n","\n","Our Query 3 :\n","==> Average Precision@20: 0.9666666666666666\n","\n","Our Query 4 :\n","==> Average Precision@20: 1.0\n","\n","Our Query 5 :\n","==> Average Precision@20: 0.9666666666666666\n","\n"]}]},{"cell_type":"markdown","source":["#**12. F1-Score**"],"metadata":{"id":"l0Q2BZD0xgiP"}},{"cell_type":"code","source":["# for each query\n","for q in range(0, len(queries)):\n","  print(\"Query\", q + 1, \":\") # print the query number\n","  p = precision_at_k(ev_gt[q][\"label\"], ev_gt[q][\"score\"], k) # compute once again the precision\n","  r = len(ev_gt[q][ev_gt[q][\"label\"] == 1])/len(docs_arr[q]) # compute once again the recall\n","  f1_score = (1**2 + 1) * (r * p) / (r + 1**2 * p) # calculate the f1 score\n","\n","  print(\"==> F1-Score: {}\\n\".format(f1_score)) # print the f1 score result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hO6s4k-5xltm","executionInfo":{"status":"ok","timestamp":1667932565364,"user_tz":-60,"elapsed":25,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"02037539-d16f-43c4-b18c-ff0e27effe17"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Query 1 :\n","==> F1-Score: 0.1652892561983471\n","\n","Query 2 :\n","==> F1-Score: 0.7142857142857143\n","\n","Query 3 :\n","==> F1-Score: 0.425531914893617\n","\n"]}]},{"cell_type":"code","source":["# repeat the f1-score calculations for our queries\n","# for each query\n","for q in range(0, len(our_queries)):\n","  print(\"Our Query\", q + 1, \":\") # print the query number\n","  p = precision_at_k(ev_gt_our_q[q][\"label\"], ev_gt_our_q[q][\"score\"], k) # compute once again the precision\n","  r = len(ev_gt_our_q[q][ev_gt_our_q[q][\"label\"] == 1])/len(docs_arr_our_queries[q]) # compute once again the recall\n","  f1_score = (1**2 + 1) * (r * p) / (r + 1**2 * p) # calculate the f1 score\n","\n","  print(\"==> F1-Score: {}\\n\".format(f1_score)) # print the f1 score result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQMBflJ82UeW","executionInfo":{"status":"ok","timestamp":1667932565364,"user_tz":-60,"elapsed":22,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"4e8a71a3-d129-48c4-90e5-a3b48f1c2826"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Query 1 :\n","==> F1-Score: 0.009251700680272108\n","\n","Our Query 2 :\n","==> F1-Score: 0.006578947368421053\n","\n","Our Query 3 :\n","==> F1-Score: 0.023148148148148147\n","\n","Our Query 4 :\n","==> F1-Score: 0.006622516556291391\n","\n","Our Query 5 :\n","==> F1-Score: 0.023310023310023312\n","\n"]}]},{"cell_type":"markdown","source":["#**13. Mean Average Precision**"],"metadata":{"id":"k2vo5sFExl9f"}},{"cell_type":"code","source":["def map_at_k(search_res, k=10): #receives all the search results dataframe containing all the queries and the results and relevances\n","    \"\"\"\n","    Parameters\n","    ----------\n","    search_res: search results dataset containing:\n","        query_id: query id.\n","        doc_id: document id.\n","        label: relevance predicted through LightGBM.\n","        score: actual score of the document for the query (ground truth).\n","\n","    Returns\n","    -------\n","    mean average precision @ k : float\n","    \"\"\"\n","    avp = []\n","    for q in search_res[\"query_id\"].unique():  # loop over all query ids\n","        curr_data = search_res[search_res[\"query_id\"] == q]  # select data for current query (get a slice of the dataframe keeping only the data related to the current query)\n","        avp.append(avg_precision_at_k(np.array(curr_data[\"label\"]), \n","                   np.array(curr_data[\"score\"]), k))  #append average precision for current query\n","    return np.sum(avp) / len(avp), avp  # return mean average precision"],"metadata":{"id":"DueDfwfMyOVl","executionInfo":{"status":"ok","timestamp":1667932565365,"user_tz":-60,"elapsed":18,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["evaluation_gt = pd.concat(ev_gt) # aggrupate the different ev_gt in a dataframe\n","map_k, avp = map_at_k(evaluation_gt, 20) # calculate the mean average precission @k\n","print(\"==> Mean Average Precission@{}: {}\\n\".format(20, map_k)) # print the map @k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALegWY5vDqrG","executionInfo":{"status":"ok","timestamp":1667932565365,"user_tz":-60,"elapsed":17,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"ede9a48f-48d8-4931-f56b-c0bd0a3fd97b"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Mean Average Precission@20: 0.975448532948533\n","\n"]}]},{"cell_type":"code","source":["# repeat the map_k calculations for our queries\n","evaluation_gt_our_queries = pd.concat(ev_gt_our_q) # aggrupate the different ev_gt in a dataframe\n","map_k, avp = map_at_k(evaluation_gt_our_queries, 20) # calculate the mean average precission @k\n","print(\"==> Mean Average Precission@{}: {}\\n\".format(20, map_k)) # print the map @k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rTA0h3mp2kWz","executionInfo":{"status":"ok","timestamp":1667932565365,"user_tz":-60,"elapsed":14,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"e69c2931-8522-4809-ec76-b1d94365dbf7"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Mean Average Precission@20: 0.9479841600051945\n","\n"]}]},{"cell_type":"markdown","source":["#**14. Mean Reciprocal Rank**"],"metadata":{"id":"OhQQJ1mmyOkw"}},{"cell_type":"code","source":["def rr_at_k(doc_score, y_score, k=10):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    doc_score: Ground truth (true relevance labels).\n","    y_score: Predicted scores.\n","    k : number of doc to consider.\n","\n","    Returns\n","    -------\n","    Reciprocal Rank for current query\n","    \"\"\"\n","\n","    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order. As before\n","    doc_score = np.take(doc_score, order[\n","                             :k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k. As before\n","    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n","        return 0\n","    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\" (+1 because the idex starts from 0)"],"metadata":{"id":"Am8JYkK_yb4r","executionInfo":{"status":"ok","timestamp":1667932565366,"user_tz":-60,"elapsed":12,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["mrr = {}\n","for k in [5, 10, 15, 20]:\n","  RRs = []\n","  # for each query\n","  for q in range(0, len(queries)):\n","    rr = rr_at_k(np.array(ev_gt[q][\"label\"]), np.array(ev_gt[q][\"score\"]), k)\n","    RRs.append(rr) # append RR for current query\n","    print(\"==> Reciprocal Rank at k = {} for query q = {}: {}\\n\".format(k, q + 1, rr))\n","  mrr[k] = np.round(float(sum(RRs) / len(RRs)), 4)  # Mean RR at current k\n","  print(\"==> Mean Reciprocal Rank at k = {}: {}\\n\".format(k, mrr[k]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRW1GuSvKy8z","executionInfo":{"status":"ok","timestamp":1667932565699,"user_tz":-60,"elapsed":345,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"167381f2-709d-446f-f5a5-b1356fd694d4"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Reciprocal Rank at k = 5 for query q = 1: 1.0\n","\n","==> Reciprocal Rank at k = 5 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 5 for query q = 3: 1.0\n","\n","==> Mean Reciprocal Rank at k = 5: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 1: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 3: 1.0\n","\n","==> Mean Reciprocal Rank at k = 10: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 1: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 3: 1.0\n","\n","==> Mean Reciprocal Rank at k = 15: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 1: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 3: 1.0\n","\n","==> Mean Reciprocal Rank at k = 20: 1.0\n","\n"]}]},{"cell_type":"code","source":["# repeat the mrr calculations for our queries\n","mrr = {}\n","for k in [5, 10, 15, 20]:\n","  RRs = []\n","  # for each query\n","  for q in range(0, len(our_queries)):\n","    rr = rr_at_k(np.array(ev_gt_our_q[q][\"label\"]), np.array(ev_gt_our_q[q][\"score\"]), k)\n","    RRs.append(rr) # append RR for current query\n","    print(\"==> Reciprocal Rank at k = {} for query q = {}: {}\\n\".format(k, q + 1, rr))\n","  mrr[k] = np.round(float(sum(RRs) / len(RRs)), 4)  # Mean RR at current k\n","  print(\"==> Mean Reciprocal Rank at k = {}: {}\\n\".format(k, mrr[k]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMTr1Iy-202L","executionInfo":{"status":"ok","timestamp":1667932565700,"user_tz":-60,"elapsed":18,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"4ae9ab12-f070-40be-c78d-61c94b2e4444"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Reciprocal Rank at k = 5 for query q = 1: 0.5\n","\n","==> Reciprocal Rank at k = 5 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 5 for query q = 3: 1.0\n","\n","==> Reciprocal Rank at k = 5 for query q = 4: 1.0\n","\n","==> Reciprocal Rank at k = 5 for query q = 5: 1.0\n","\n","==> Mean Reciprocal Rank at k = 5: 0.9\n","\n","==> Reciprocal Rank at k = 10 for query q = 1: 0.5\n","\n","==> Reciprocal Rank at k = 10 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 3: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 4: 1.0\n","\n","==> Reciprocal Rank at k = 10 for query q = 5: 1.0\n","\n","==> Mean Reciprocal Rank at k = 10: 0.9\n","\n","==> Reciprocal Rank at k = 15 for query q = 1: 0.5\n","\n","==> Reciprocal Rank at k = 15 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 3: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 4: 1.0\n","\n","==> Reciprocal Rank at k = 15 for query q = 5: 1.0\n","\n","==> Mean Reciprocal Rank at k = 15: 0.9\n","\n","==> Reciprocal Rank at k = 20 for query q = 1: 0.5\n","\n","==> Reciprocal Rank at k = 20 for query q = 2: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 3: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 4: 1.0\n","\n","==> Reciprocal Rank at k = 20 for query q = 5: 1.0\n","\n","==> Mean Reciprocal Rank at k = 20: 0.9\n","\n"]}]},{"cell_type":"markdown","source":["#**15. Normalized Discounted Cumulative Gain**"],"metadata":{"id":"vlQKzfRWycSl"}},{"cell_type":"code","source":["def dcg_at_k(doc_score, y_score, k=10): #doc_scire are the labels (ground truth) and y_score are the system scores\n","    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n","    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n","    gain = 2 ** doc_score - 1  # First we calculate the upper part of the formula which is the CG (use formula 7 above) (notice it is based on the ground truth relevance)\n","    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator (np.arrange creates a list of numbers betweeen 0 and len(doc_score)-1), then the + 2 addresses the fact that the numbers start from 0\n","    return np.sum(gain / discounts)  #return dcg@k\n","\n","\n","def ndcg_at_k(doc_score, y_score, k=10):\n","    dcg_max = dcg_at_k(doc_score, doc_score, k) #ideal dcg\n","    #print(dcg_max)\n","    if not dcg_max:\n","        return 0\n","    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"],"metadata":{"id":"nLWnQDIdykuP","executionInfo":{"status":"ok","timestamp":1667932565700,"user_tz":-60,"elapsed":15,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# for each query\n","for q in range(0, len(queries)):\n","  k = len(ev_gt[q])\n","  ndcg_k = np.round(ndcg_at_k(ev_gt[q][\"label\"], ev_gt[q][\"score\"], k), 4) # calculate the ndcg\n","  print(\"==> ndcg@{} for query with query_id={}: {}\".format(k, q + 1, ndcg_k)) # print the ncdg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUFrP7QONRwA","executionInfo":{"status":"ok","timestamp":1667932565701,"user_tz":-60,"elapsed":16,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"e8acd029-8ebb-40f1-890d-4440331442f7"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["==> ndcg@20 for query with query_id=1: 1.0\n","==> ndcg@20 for query with query_id=2: 0.9762\n","==> ndcg@20 for query with query_id=3: 1.0\n"]}]},{"cell_type":"code","source":["# for each query\n","for q in range(0, len(queries)):\n","  ndcgs = []\n","  k = len(ev_gt[q])\n","  ndcgs.append(np.round(ndcg_at_k(ev_gt[q][\"label\"], ev_gt[q][\"score\"], k), 4)) # append NDCG\n","\n","avg_ndcg = np.round(float(sum(ndcgs) / len(ndcgs)), 4) # calculate the average\n","print(\"Average ndcg@{}: {}\".format(k, avg_ndcg)) # print the average ndcg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTC71ifiNVm9","executionInfo":{"status":"ok","timestamp":1667932565701,"user_tz":-60,"elapsed":13,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"7710db2a-c2fd-4a95-98ea-ed0a97e9e73f"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["Average ndcg@20: 1.0\n"]}]},{"cell_type":"code","source":["# repeat the ndcg calculations for our queries\n","for q in range(0, len(our_queries)):\n","  k = len(ev_gt_our_q[q])\n","  ndcg_k = np.round(ndcg_at_k(ev_gt_our_q[q][\"label\"], ev_gt_our_q[q][\"score\"], k), 4)\n","  print(\"==> ndcg@{} for query with query_id={}: {}\".format(k, q + 1, ndcg_k))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTpvda3t3bWY","executionInfo":{"status":"ok","timestamp":1667932565701,"user_tz":-60,"elapsed":11,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"7354be09-87a5-4daf-98ba-a29d3cad1786"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["==> ndcg@20 for query with query_id=1: 0.8676\n","==> ndcg@20 for query with query_id=2: 1.0\n","==> ndcg@20 for query with query_id=3: 0.9896\n","==> ndcg@20 for query with query_id=4: 1.0\n","==> ndcg@20 for query with query_id=5: 0.9896\n"]}]},{"cell_type":"code","source":["# repeat the average ndcg calculations for our queries\n","for q in range(0, len(our_queries)):\n","  ndcgs = []\n","  k = len(ev_gt_our_q[q])\n","  ndcgs.append(np.round(ndcg_at_k(ev_gt_our_q[q][\"label\"], ev_gt_our_q[q][\"score\"], k), 4))\n","\n","avg_ndcg = np.round(float(sum(ndcgs) / len(ndcgs)), 4)\n","print(\"Average ndcg@{}: {}\".format(k, avg_ndcg))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxEHLRaM3cKy","executionInfo":{"status":"ok","timestamp":1667932565703,"user_tz":-60,"elapsed":10,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"a40cea3d-972b-49cc-9711-d83656c57bbb"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["Average ndcg@20: 0.9896\n"]}]},{"cell_type":"markdown","source":["#**16. Vector Representation**"],"metadata":{"id":"jq1Pf0-Fyk4z"}},{"cell_type":"code","source":["tweets = []\n","\n","# create an array containing all the tweets text pre-processed\n","for i in tweets_index:\n","  tweets.append(tweets_index[i][\"normalised_text\"])"],"metadata":{"id":"sU_UGvnaUH9g","executionInfo":{"status":"ok","timestamp":1667932565704,"user_tz":-60,"elapsed":9,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["# calculate our model using the Word2Vec\n","model = Word2Vec(tweets, workers=4, size=100, min_count=50, window=10, sample=1e-3)\n","\n","# print the most similar results to hurrican for checking if our model works\n","print(model.wv.most_similar('hurrican'))\n","\n","X = model.wv[model.wv.vocab]\n","\n","# calculate the TSNE\n","tsne = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n","X_tsne = tsne.fit_transform(X)\n","\n","# plot the scatter plot with the results\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"sDKNhLLcywyT","executionInfo":{"status":"ok","timestamp":1667932570714,"user_tz":-60,"elapsed":5018,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"11a03d5a-bae1-4f00-e362-8408d2124448"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n","/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["[('show', 0.9973011612892151), ('damag', 0.9972115159034729), ('live', 0.9970601797103882), ('come', 0.9967733025550842), ('ian', 0.9966986179351807), ('continu', 0.9966046810150146), ('take', 0.996536135673523), ('area', 0.9964593648910522), ('watch', 0.996441125869751), ('pray', 0.9964300990104675)]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}