{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHbW3Gc/iFModpDPWR1boc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbHIob8C6qlq","executionInfo":{"status":"ok","timestamp":1666288360900,"user_tz":-120,"elapsed":20179,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"80fff0cd-aee3-4eba-ce05-35362503c399"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount the drive for latter importing the datasets\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#**1. Import dictionaries**"],"metadata":{"id":"iUdATAJvjf0w"}},{"cell_type":"code","source":["# download nltk and the stopwords\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2UjO36z7dDl","executionInfo":{"status":"ok","timestamp":1666288361811,"user_tz":-120,"elapsed":919,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"bc00628f-e810-47a8-9152-bc63f331ead2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# download the rest of dictionaries\n","from collections import defaultdict\n","from array import array\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import math\n","import numpy as np\n","import collections\n","from numpy import linalg as la\n","import time\n","\n","# We have added these dictionaries to the ones that were included in lab 1\n","import json\n","import re\n","import csv"],"metadata":{"id":"S4Ja2y3-7iWL","executionInfo":{"status":"ok","timestamp":1666288361812,"user_tz":-120,"elapsed":9,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#**2. Load the datasets**"],"metadata":{"id":"UBOrO5Gzj0DT"}},{"cell_type":"code","source":["docs_path = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tw_hurricane_data.json'\n","tweets = []\n","# open the JSON file\n","with open(docs_path) as fp:\n","    for jsonObj in fp:\n","        tweetsDict = json.loads(jsonObj)\n","        tweets.append(tweetsDict) # add the tweets in our array tweets"],"metadata":{"id":"6CylZgIYM9b4","executionInfo":{"status":"ok","timestamp":1666288363808,"user_tz":-120,"elapsed":2004,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["docs_path_2 = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tweet_document_ids_map.csv'\n","doc_id = {}\n","# open the CSV file\n","with open(docs_path_2, newline='') as csvfile:\n","  spamreader = csv.reader(csvfile, delimiter=' ', quotechar=' ')\n","  for row in spamreader:\n","    doc_id[row[0].split()[1]] = row[0].split()[0] # add the doc number as an entry of our dictionary, having the tweet id as the key of this entry"],"metadata":{"id":"wNoB7lcV7j15","executionInfo":{"status":"ok","timestamp":1666288363810,"user_tz":-120,"elapsed":24,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#**3. Text Processing**"],"metadata":{"id":"YmAgPsFwkUox"}},{"cell_type":"code","source":["def build_terms(tweet):\n","    \"\"\"\n","    Preprocess the text of the tweet by eliminating the url, the people labelled with the @,\n","    eliminating the punctuation, separating the words after the hashtag, removing stop words, \n","    stemming, transforming in lowercase and returning the tokens of the text.\n","    \n","    Argument:\n","    tweet -- string (text) to be pre-processed\n","    \n","    Returns:\n","    tweet - a list of tokens corresponding to the input text after the pre-processing\n","    \"\"\"\n","\n","    stemmer = PorterStemmer() # stemm the words to get the root of the word and avoid having different words that mean the same\n","    stop_words = set(stopwords.words(\"english\")) # eliminate all the stop words to make efficient queries and documents\n","    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # separate the words without including puntuation marks\n","    \n","    tweet = re.sub(r'http\\S+', '', tweet) ## delete the url\n","    tweet = re.sub(r'@\\S+', '', tweet) ## delete the word after @ (so the people labelled)\n","    tweet = \" \".join([a for a in re.split('([A-Z][a-z]+)', tweet) if a]) ## separate the hashtags in words according to the capital letters\n","    tweet = tweet.replace(\"_\", \" \") ## eliminate the _ (it is the only punctuation mark that is not deleted with tokenize)\n","    tweet = tweet.lower() ## transform in lowercase\n","    tweet = tokenizer.tokenize(tweet) ## tokenize the text to get a list of terms and remove punctuation marks\n","    tweet=[i for i in tweet if i not in stop_words]  ## eliminate the stopwords\n","    tweet=[stemmer.stem(i) for i in tweet] ## perform stemming\n","\n","    return tweet"],"metadata":{"id":"y4OC8L7s_7Oc","executionInfo":{"status":"ok","timestamp":1666288363811,"user_tz":-120,"elapsed":20,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def create_index(tweets):\n","  \"\"\"\n","  Create the inverted index and the tweets dictionary\n","\n","  Argument:\n","  tweets -- collection of tweets\n","  \n","  Returns:\n","  index -- the inverted index. Contains the terms as keys and in which tweets (appear as the document number related to the tweet id)\n","  and in which position inside this tweet appears each term\n","  tweets-index -- the tweet's dictionary. Contains an entry for each tweet which key is the document number related with\n","  the tweet's id. Each tweet has its text, username, date, hashtags, number of likes, number of retweets and url if they exist \n","  \"\"\"\n","  index = defaultdict(list) # We create the inverted index\n","  tweets_index = {} # We create the tweets dictionary\n","  counter = 0\n","\n","  for tweet in tweets:\n","    # for each tweet we create a dictionary containing the text, username, date, hashtags, number of likes, number of retweets and url if they exist\n","    tweet_dict = {}\n","    try:\n","      tweet_dict[\"text\"] = tweet[\"full_text\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"username\"] = tweet['user']['screen_name']\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"date\"] = tweet[\"created_at\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"hashtags\"] = []\n","      for i in range(0, len(tweet[\"entities\"][\"hashtags\"])):\n","        tweet_dict[\"hashtags\"].append(tweet[\"entities\"][\"hashtags\"][i])\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"likes\"] = tweet[\"favorite_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"retweets\"] = tweet[\"retweet_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"url\"] = tweet[\"entities\"][\"media\"][0][\"url\"]\n","    except:\n","      pass\n","\n","    tweets_index[doc_id[str(tweet[\"id\"])]] = tweet_dict # save the tweet in tweets index by the document number related with the tweet id\n","\n","    terms = build_terms(tweet[\"full_text\"]) # call build terms for processing the text of the tweet\n","\n","    if counter <= 10:\n","      # print the tweet text and terms for checking it the result is okay (now we only do this for the first tweet but before delivering we have checked more tweets)\n","      print(\"Original full text of the tweet': \\n{}\".format(tweet[\"full_text\"]))\n","      print(\"Terms after processing the text': \\n{}\".format(terms))\n","      counter += 1\n","\n","    current_page_index = {}\n","\n","    for position, term in enumerate(terms): # loop over all terms\n","        try:\n","            # if the term is already in the index for the current page append the position\n","            current_page_index[term][1].append(position)\n","        except:\n","            # else add the new term as dict key and set the document number corresponding to this tweet and the position where the term appears in this tweet\n","            current_page_index[term]=[doc_id[str(tweet[\"id\"])], array('I',[position])] #'I' indicates unsigned int (int in Python)\n","        \n","    #merge the current page index with the main index\n","    for term_page, posting_page in current_page_index.items():\n","        index[term_page].append(posting_page)\n","    \n","  return index, tweets_index"],"metadata":{"id":"GZxG9cENPTlM","executionInfo":{"status":"ok","timestamp":1666288363812,"user_tz":-120,"elapsed":19,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#**4. Check the results**"],"metadata":{"id":"X0UIXhCGZJzL"}},{"cell_type":"code","source":["start_time = time.time()\n","index, tweets_index = create_index(tweets) # run create_index() for creating the inverted index and the tweets index\n","print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2))) # calculate how much time does the process last"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-ZvsfJHaKF9","executionInfo":{"status":"ok","timestamp":1666288365958,"user_tz":-120,"elapsed":2161,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"dfa9ef1d-ac86-4f26-a29f-e1470a6c1b0f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original full text of the tweet': \n","So this will keep spinning over us until 7 pm‚Ä¶go away already. #HurricaneIan https://t.co/VROTxNS9rz\n","Terms after processing the text': \n","['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', 'hurrican', 'ian']\n","Original full text of the tweet': \n","Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. üíô\n","Terms after processing the text': \n","['heart', 'go', 'affect', 'hurrican', 'ian', 'wish', 'everyon', 'road', 'current', 'brave', 'condit', 'safe', 'travel']\n","Original full text of the tweet': \n","Kissimmee neighborhood off of Michigan Ave. \n","#HurricaneIan https://t.co/jf7zseg0Fe\n","Terms after processing the text': \n","['kissimme', 'neighborhood', 'michigan', 'ave', 'hurrican', 'ian']\n","Original full text of the tweet': \n","I have this one tree in my backyard that scares me more than the poltergeist tree when it‚Äôs storming and windy like this. #scwx #HurricaneIan\n","Terms after processing the text': \n","['one', 'tree', 'backyard', 'scare', 'poltergeist', 'tree', 'storm', 'windi', 'like', 'scwx', 'hurrican', 'ian']\n","Original full text of the tweet': \n","@AshleyRuizWx @Stephan89441722 @lilmizzheidi @Mr__Sniffles @winknews @DylanFedericoWX @julianamwx @sydneypersing @NicoleGabeTV I pray for everyone affected by #HurricaneIan, but not those associated with WINKnews.  No sympathy for animal abusers, liars, and those that condone it.\n","Terms after processing the text': \n","['pray', 'everyon', 'affect', 'hurrican', 'ian', 'associ', 'win', 'knew', 'sympathi', 'anim', 'abus', 'liar', 'condon']\n","Original full text of the tweet': \n","Ace Handyman Services hopes everyone was safe during the Hurricane. Any damages caused by the hurricane is our first priority! Call and schedule an appointment with one of our multi-skilled craftsmen today! üìû813-565-2022\n","#HurricaneIan #AHS #BringingHelpfulToYourHome https://t.co/BfpOq7tJE0\n","Terms after processing the text': \n","['ace', 'handyman', 'servic', 'hope', 'everyon', 'safe', 'hurrican', 'damag', 'caus', 'hurrican', 'first', 'prioriti', 'call', 'schedul', 'appoint', 'one', 'multi', 'skill', 'craftsmen', 'today', '813', '565', '2022', 'hurrican', 'ian', 'ah', 'bring', 'help', 'home']\n","Original full text of the tweet': \n","Storm surge issues in Georgetown, SC #HurricaneIan https://t.co/qWs0XJzGMx\n","Terms after processing the text': \n","['storm', 'surg', 'issu', 'georgetown', 'sc', 'hurrican', 'ian']\n","Original full text of the tweet': \n","Our thoughts are with the students, teachers, parents, and communities, suffering in the wake of Hurricane Ian. \n","\n","#CloseUpDC #HurricaneIan #Florida #Georgia #NorthCarolina #SouthCarolina https://t.co/eHZ9NKhCgA\n","Terms after processing the text': \n","['thought', 'student', 'teacher', 'parent', 'commun', 'suffer', 'wake', 'hurrican', 'ian', 'close', 'dc', 'hurrican', 'ian', 'florida', 'georgia', 'north', 'carolina', 'south', 'carolina']\n","Original full text of the tweet': \n","#SouthCarolina braces for #HurricaneIan to make landfall within HOURS https://t.co/d9sZsk0atW via @MailOnline\n","Terms after processing the text': \n","['south', 'carolina', 'brace', 'hurrican', 'ian', 'make', 'landfal', 'within', 'hour', 'via']\n","Original full text of the tweet': \n","How pissed is GOD to send #HurricaneIan to Florida and South Carolina!?\n","\n","The #MAGA cult has angered GOD and are paying for their sins.\n","\n","@RonDeSantisFL @scgovernorpress #Florida #SouthCarolina #MyrtleBeach\n","Terms after processing the text': \n","['piss', 'god', 'send', 'hurrican', 'ian', 'florida', 'south', 'carolina', 'maga', 'cult', 'anger', 'god', 'pay', 'sin', 'florida', 'south', 'carolina', 'myrtl', 'beach']\n","Original full text of the tweet': \n","Today's edition of The Smoke Eater is a photo essay on #HurricaneIan.\n","\n","Also, there are animal bleps.\n","\n","https://t.co/ZwjBziC9jf\n","Terms after processing the text': \n","['today', 'edit', 'smoke', 'eater', 'photo', 'essay', 'hurrican', 'ian', 'also', 'anim', 'blep']\n","Total time to create the index: 1.93 seconds\n"]}]},{"cell_type":"code","source":["# check the first index results for a term\n","print(\"Index results for the term 'hurricane': {}\\n\".format(index['hurricane']))\n","print(\"First 10 Index results for the term 'hurrican': \\n{}\".format(index['hurrican'][:10]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb13xRsvp2mT","executionInfo":{"status":"ok","timestamp":1666288365960,"user_tz":-120,"elapsed":38,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"612cb537-1427-4af8-c96e-282f9e75177d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Index results for the term 'hurricane': []\n","\n","First 10 Index results for the term 'hurrican': \n","[['doc_1', array('I', [8])], ['doc_2', array('I', [3])], ['doc_3', array('I', [4])], ['doc_4', array('I', [10])], ['doc_5', array('I', [3])], ['doc_6', array('I', [6, 9, 23])], ['doc_7', array('I', [5])], ['doc_8', array('I', [7, 11])], ['doc_9', array('I', [3])], ['doc_10', array('I', [3])]]\n"]}]},{"cell_type":"code","source":["# check the tweets index result for a tweet\n","print(tweets_index[\"doc_4\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Op_S1BdIV0oc","executionInfo":{"status":"ok","timestamp":1666288365966,"user_tz":-120,"elapsed":29,"user":{"displayName":"√ÄLEX GARC√çA MONTAN√â","userId":"17457049892642956717"}},"outputId":"c3c7180f-fb29-485d-9d33-73e96764b787"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': 'I have this one tree in my backyard that scares me more than the poltergeist tree when it‚Äôs storming and windy like this. #scwx #HurricaneIan', 'username': 'spiralgypsy', 'date': 'Fri Sep 30 18:38:57 +0000 2022', 'hashtags': [{'text': 'scwx', 'indices': [122, 127]}, {'text': 'HurricaneIan', 'indices': [128, 141]}], 'likes': 0, 'retweets': 0}\n"]}]}]}