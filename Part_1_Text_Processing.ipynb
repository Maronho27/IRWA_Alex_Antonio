{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMS4hRV9HaH/iHU3FzU4oU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbHIob8C6qlq","executionInfo":{"status":"ok","timestamp":1665758076939,"user_tz":-120,"elapsed":27599,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}},"outputId":"84d37804-3998-437f-aaa1-1511f3784a88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount the drive for latter importing the datasets\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#**1. Import dictionaries**"],"metadata":{"id":"iUdATAJvjf0w"}},{"cell_type":"code","source":["# download nltk and the stopwords\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2UjO36z7dDl","executionInfo":{"status":"ok","timestamp":1665758078379,"user_tz":-120,"elapsed":1450,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}},"outputId":"b5bd0e0c-335d-4832-ee5d-b41804fd7b71"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# download the rest of dictionaries\n","from collections import defaultdict\n","from array import array\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import math\n","import numpy as np\n","import collections\n","from numpy import linalg as la\n","import time\n","\n","# We have added these dictionaries to the ones that were included in lab 1\n","import json\n","import re\n","import csv"],"metadata":{"id":"S4Ja2y3-7iWL","executionInfo":{"status":"ok","timestamp":1665758078380,"user_tz":-120,"elapsed":12,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#**2. Load the datasets**"],"metadata":{"id":"UBOrO5Gzj0DT"}},{"cell_type":"code","source":["docs_path = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tw_hurricane_data.json'\n","tweets = []\n","# open the JSON file\n","with open(docs_path) as fp:\n","    for jsonObj in fp:\n","        tweetsDict = json.loads(jsonObj)\n","        tweets.append(tweetsDict) # add the tweets in our array tweets"],"metadata":{"id":"6CylZgIYM9b4","executionInfo":{"status":"ok","timestamp":1665758080897,"user_tz":-120,"elapsed":2525,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["docs_path_2 = 'drive/MyDrive/IRWA/Part_1:Text_Processing/Hurricane_Ian_Corpus/data/tweet_document_ids_map.csv'\n","doc_id = {}\n","# open the CSV file\n","with open(docs_path_2, newline='') as csvfile:\n","  spamreader = csv.reader(csvfile, delimiter=' ', quotechar=' ')\n","  for row in spamreader:\n","    doc_id[row[0].split()[1]] = row[0].split()[0] # add the doc number as an entry of our dictionary, having the tweet id as the key of this entry"],"metadata":{"id":"wNoB7lcV7j15","executionInfo":{"status":"ok","timestamp":1665758082300,"user_tz":-120,"elapsed":1414,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#**3. Text Processing**"],"metadata":{"id":"YmAgPsFwkUox"}},{"cell_type":"code","source":["def build_terms(tweet):\n","    \"\"\"\n","    Preprocess the text of the tweet by eliminating the url, the people labelled with the @,\n","    eliminating the punctuation, separating the words after the hashtag, removing stop words, \n","    stemming, transforming in lowercase and returning the tokens of the text.\n","    \n","    Argument:\n","    tweet -- string (text) to be pre-processed\n","    \n","    Returns:\n","    tweet - a list of tokens corresponding to the input text after the pre-processing\n","    \"\"\"\n","\n","    stemmer = PorterStemmer() # stemm the words to get the root of the word and avoid having different words that mean the same\n","    stop_words = set(stopwords.words(\"english\")) # eliminate all the stop words to make efficient queries and documents\n","    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # separate the words without including puntuation marks\n","    \n","    tweet = re.sub(r'http\\S+', '', tweet) ## delete the url\n","    tweet = re.sub(r'@\\S+', '', tweet) ## delete the word after @ (so the people labelled)\n","    tweet = \" \".join([a for a in re.split('([A-Z][a-z]+)', tweet) if a]) ## separate the hashtags in words according to the capital letters\n","    tweet = tweet.replace(\"_\", \" \") ## eliminate the _ (it is the only punctuation mark that is not deleted with tokenize)\n","    tweet = tweet.lower() ## transform in lowercase\n","    tweet = tokenizer.tokenize(tweet) ## tokenize the text to get a list of terms and remove punctuation marks\n","    tweet=[i for i in tweet if i not in stop_words]  ## eliminate the stopwords\n","    tweet=[stemmer.stem(i) for i in tweet] ## perform stemming\n","\n","    return tweet"],"metadata":{"id":"y4OC8L7s_7Oc","executionInfo":{"status":"ok","timestamp":1665758082302,"user_tz":-120,"elapsed":18,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def create_index(tweets):\n","  \"\"\"\n","  Create the inverted index and the tweets dictionary\n","\n","  Argument:\n","  tweets -- collection of tweets\n","  \n","  Returns:\n","  index -- the inverted index. Contains the terms as keys and in which tweets (appear as the document number related to the tweet id)\n","  and in which position inside this tweet appears each term\n","  tweets-index -- the tweet's dictionary. Contains an entry for each tweet which key is the document number related with\n","  the tweet's id. Each tweet has its text, username, date, hashtags, number of likes, number of retweets and url if they exist \n","  \"\"\"\n","  index = defaultdict(list) # We create the inverted index\n","  tweets_index = {} # We create the tweets dictionary\n","  identifier = 0\n","\n","  for tweet in tweets:\n","    # for each tweet we create a dictionary containing the text, username, date, hashtags, number of likes, number of retweets and url if they exist\n","    tweet_dict = {}\n","    try:\n","      tweet_dict[\"text\"] = tweet[\"full_text\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"username\"] = tweet['user']['screen_name']\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"date\"] = tweet[\"created_at\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"hashtags\"] = []\n","      for i in range(0, len(tweet[\"entities\"][\"hashtags\"])):\n","        tweet_dict[\"hashtags\"].append(tweet[\"entities\"][\"hashtags\"][i])\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"likes\"] = tweet[\"favorite_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"retweets\"] = tweet[\"retweet_count\"]\n","    except:\n","      pass\n","    try:\n","      tweet_dict[\"url\"] = tweet[\"entities\"][\"media\"][0][\"url\"]\n","    except:\n","      pass\n","\n","    tweets_index[doc_id[str(tweet[\"id\"])]] = tweet_dict # save the tweet in tweets index by the document number related with the tweet id\n","\n","    terms = build_terms(tweet[\"full_text\"]) # call build terms for processing the text of the tweet\n","\n","    if identifier == 0:\n","      # print the tweet text and terms for checking it the result is okay (now we only do this for the first tweet but before delivering we have checked more tweets)\n","      print(tweet[\"full_text\"])\n","      print(terms)\n","      identifier = 1\n","\n","    current_page_index = {}\n","\n","    for position, term in enumerate(terms): # loop over all terms\n","        try:\n","            # if the term is already in the index for the current page append the position\n","            current_page_index[term][1].append(position)\n","        except:\n","            # else add the new term as dict key and set the document number corresponding to this tweet and the position where the term appears in this tweet\n","            current_page_index[term]=[doc_id[str(tweet[\"id\"])], array('I',[position])] #'I' indicates unsigned int (int in Python)\n","        \n","    #merge the current page index with the main index\n","    for term_page, posting_page in current_page_index.items():\n","        index[term_page].append(posting_page)\n","    \n","  return index, tweets_index"],"metadata":{"id":"GZxG9cENPTlM","executionInfo":{"status":"ok","timestamp":1665758082303,"user_tz":-120,"elapsed":16,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#**4. Check the results**"],"metadata":{"id":"X0UIXhCGZJzL"}},{"cell_type":"code","source":["start_time = time.time()\n","index, tweets_index = create_index(tweets) # run create_index() for creating the inverted index and the tweets index\n","print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2))) # calculate how much time does the process last"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-ZvsfJHaKF9","executionInfo":{"status":"ok","timestamp":1665758084885,"user_tz":-120,"elapsed":2596,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}},"outputId":"336669e2-02f3-41d5-e535-e7500d10f3ca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["So this will keep spinning over us until 7 pm…go away already. #HurricaneIan https://t.co/VROTxNS9rz\n","['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', 'hurrican', 'ian']\n","Total time to create the index: 2.66 seconds\n"]}]},{"cell_type":"code","source":["# check the first index results for a term\n","print(\"Index results for the term 'hurricane': {}\\n\".format(index['hurricane']))\n","print(\"First 10 Index results for the term 'hurrican': \\n{}\".format(index['hurrican'][:10]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb13xRsvp2mT","executionInfo":{"status":"ok","timestamp":1665758084887,"user_tz":-120,"elapsed":27,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}},"outputId":"23a8e7f5-88e5-49d7-eda8-6db62c8bc902"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Index results for the term 'hurricane': []\n","\n","First 10 Index results for the term 'hurrican': \n","[['doc_1', array('I', [8])], ['doc_2', array('I', [3])], ['doc_3', array('I', [4])], ['doc_4', array('I', [10])], ['doc_5', array('I', [3])], ['doc_6', array('I', [6, 9, 23])], ['doc_7', array('I', [5])], ['doc_8', array('I', [7, 11])], ['doc_9', array('I', [3])], ['doc_10', array('I', [3])]]\n"]}]},{"cell_type":"code","source":["# check the tweets index result for a tweet\n","print(tweets_index[\"doc_4\"][\"username\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Op_S1BdIV0oc","executionInfo":{"status":"ok","timestamp":1665758084889,"user_tz":-120,"elapsed":23,"user":{"displayName":"ÀLEX GARCÍA","userId":"17457049892642956717"}},"outputId":"ea4af61d-0829-4c17-c03b-d5702f1f8b84"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["spiralgypsy\n"]}]}]}